<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="Shreya Singh" />
    <meta name="description" content="Describe your website">
    <link rel="shortcut icon" type="image/x-icon" href="../img/favicon.ico">
    <title>Project 2: Modeling, Testing, and Predicting</title>
    <meta name="generator" content="Hugo 0.70.0" />
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="../css/main.css" />
    <link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" />
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,400,200bold,400old" />
    
    <!--[if lt IE 9]>
			<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
			<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
		<![endif]-->

    
  </head>

  <body>
    <div id="wrap">

      
      <nav class="navbar navbar-default">
  <div class="container">
    <div class="navbar-header">
      <a class="navbar-brand" href="../"><i class="fa fa-home"></i></a>
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <div class="navbar-collapse collapse" id="navbar">
      <ul class="nav navbar-nav navbar-right">
      
        
        <li><a href="../blog/">BLOG</a></li>
        
        <li><a href="../projects/">PROJECTS</a></li>
        
        <li><a href="../resume.pdf">RESUME</a></li>
        
      
      </ul>
    </div>
  </div>
</nav>

      
      <div class="container">
        <div class="blog-post">
          <h3>
            <strong><a href="../project2/">Project 2: Modeling, Testing, and Predicting</a></strong>
          </h3>
        </div>
        <div class="blog-title">
          <h4>
          May 14, 2020
            &nbsp;&nbsp;
            
          </h4>
        </div>
        <div class="panel panel-default">
          <div class="panel-body">
            <div class="blogpost">
              


<pre class="r"><code>library(tidyr)
set.seed(348)
library(ggplot2)
library(sandwich)
library(lmtest)
library(tidyverse)
library(glmnet)
library(Matrix)
library(dbplyr)</code></pre>
<p>Shreya Singh (ss77555)</p>
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>This dataset concerning lung cancer had 59 observations per 8 columns. The variables included Name, Surname, Age, number of smokes (Smokes), Area of cancer per cubic centimeter (AreaQ), number of alcoholic drinks (Alkhol), location of the patient to see if they lived in the city or a rural area (Loc), and whether the patient had cancer or not (Result). Overall the entire dataset was looking at the interplay between each of these variables, aside from Name and Surname, and the effect they had on contracting lung cancer.</p>
<pre class="r"><code>lung_cancer_examples &lt;- read.csv(&quot;~/lung_cancer_examples.csv&quot;)
lc &lt;- lung_cancer_examples
location &lt;- c(&quot;City&quot;, &quot;City&quot;, &quot;Rural&quot;, &quot;Rural&quot;, &quot;City&quot;, &quot;Rural&quot;, &quot;City&quot;, &quot;Rural&quot;, &quot;City&quot;, &quot;City&quot;, &quot;City&quot;, &quot;Rural&quot;, &quot;Rural&quot;, &quot;Rural&quot;, &quot;Rural&quot;, &quot;Rural&quot;, &quot;Rural&quot;, &quot;City&quot;, &quot;City&quot;, &quot;Rural&quot;, &quot;Rural&quot;, &quot;Rural&quot;, &quot;City&quot;, &quot;City&quot;, &quot;Rural&quot;, &quot;City&quot;, &quot;City&quot;, &quot;City&quot;, &quot;City&quot;, &quot;City&quot;, &quot;City&quot;, &quot;City&quot;, &quot;City&quot;, &quot;City&quot;, &quot;City&quot;, &quot;City&quot;, &quot;Rural&quot;, &quot;Rural&quot;, &quot;Rural&quot;, &quot;City&quot;, &quot;City&quot;, &quot;Rural&quot;, &quot;City&quot;, &quot;Rural&quot;, &quot;Rural&quot;, &quot;Rural&quot;, &quot;Rural&quot;, &quot;Rural&quot;, &quot;City&quot;, &quot;City&quot;, &quot;Rural&quot;, &quot;City&quot;, &quot;City&quot;, &quot;Rural&quot;, &quot;City&quot;, &quot;Rural&quot;, &quot;City&quot;, &quot;Rural&quot;, &quot;City&quot;)

lc_new &lt;- lc %&gt;% mutate(Loc = location)</code></pre>
</div>
<div id="run-manova" class="section level1">
<h1>1. Run MANOVA</h1>
<pre class="r"><code>man1&lt;-manova(cbind(Age,Smokes,AreaQ,Alkhol)~Loc, data=lc_new)
summary(man1)</code></pre>
<pre><code>##           Df   Pillai approx F num Df den Df Pr(&gt;F)
## Loc        1 0.050286   0.7148      4     54 0.5855
## Residuals 57</code></pre>
<p>A MANOVA test was performed to see whether any of the numeric variables of ‘Age’,‘Smokes’, ‘AreaQ’, and ‘Alkhol’ showed a mean difference across levels of the categorical variable of ‘Loc’ which told the location of the patient. According to the test which did not have a p-value of less than 0.05 so it failed to reject the null hypothesis, there weren’t any significant differences in the numerical variables that differed across the location. For the assumptions of MANOVA, since there are so many and difficult to test so it would be unlikely that all of them have been met. The samples were likely random and independent observations since this was recorded at a hospital according to however had been tested for cancer. It meets multivariate normality of DVs as it has more than 25 observations per each group. The assumption of omogeneity of within-group covariance matrices as it assumes it for each DV and equal covariance between any two DVs. Linear relationships among DVs seemed acceptable and there did not appear to be extreme univariate or multivariate outliers and no multicollinearity (i.e., DVs should not be too correlated) appeared to be met.</p>
</div>
<div id="randomization-test" class="section level1">
<h1>2. Randomization test</h1>
<pre class="r"><code>Count &lt;- lc$Smokes+lc$Alkhol
lc%&gt;%group_by(lc$Result)%&gt;%summarize(means=mean(Count))%&gt;%summarize(`mean_diff:`=diff(means))</code></pre>
<pre><code>## # A tibble: 1 x 1
##   `mean_diff:`
##          &lt;dbl&gt;
## 1            0</code></pre>
<pre class="r"><code>head(perm1&lt;-data.frame(condition=lc$Result,Count=sample(Count)))</code></pre>
<pre><code>##   condition Count
## 1         1    23
## 2         1    21
## 3         0    11
## 4         0    29
## 5         1    25
## 6         0    12</code></pre>
<pre class="r"><code>perm1%&gt;%group_by(condition)%&gt;% summarize(means=mean(Count))%&gt;%summarize(`mean_diff:`=diff(means))</code></pre>
<pre><code>## # A tibble: 1 x 1
##   `mean_diff:`
##          &lt;dbl&gt;
## 1         3.77</code></pre>
<pre class="r"><code>mean(lc$Smokes)</code></pre>
<pre><code>## [1] 15.0678</code></pre>
<pre class="r"><code>mean(lc$Alkhol)</code></pre>
<pre><code>## [1] 3.237288</code></pre>
<pre class="r"><code>mean(lc$Smokes)-mean(lc$Alkhol)</code></pre>
<pre><code>## [1] 11.83051</code></pre>
<pre class="r"><code>Puffs &lt;- c(lc$Smokes)
Liquor &lt;- c(lc$Alkhol)
cancer &lt;- data.frame(count = c(Puffs, Liquor), drug = c(rep(&quot;S&quot;, 59), rep(&quot;A&quot;, 59)))
rand_dist&lt;-vector()
for(i in 1:5000){
new&lt;-data.frame(count=sample(cancer$count),drug=cancer$drug)
rand_dist[i]&lt;-new %&gt;% group_by(drug) %&gt;% summarize(means = mean(count)) %&gt;% summarize(diff(means)) %&gt;% pull}
wdiff &lt;- cancer %&gt;% group_by(drug) %&gt;% summarize(means = mean(count)) %&gt;% summarize(diff(means)) %&gt;% pull 
mean(rand_dist &gt; abs(wdiff))*2</code></pre>
<pre><code>## [1] 0</code></pre>
<pre class="r"><code>{hist(rand_dist,main=&quot;&quot;,ylab=&quot;&quot;); abline(v =11.831,col=&quot;red&quot;)}</code></pre>
<p><img src="../project2_files/figure-html/unnamed-chunk-4-1.png" width="768" style="display: block; margin: auto;" /> The null hypothesis would be the mean number of counts is the same for smoking and drinking. The alternative hypothesis would be that the mean number of counts is not the same for smoking and drinking. When utilizing the randomization test to scramble or randomize the relationship between variables in the sample to generate a null distribution against which to compare an observed test statistic, the randomized test showed a result of 0. This result would indicate that the null hypothesis would be rejected since 0 is less than the 0.05 and so the actual mean number of counts is not the same for smoking and drinking. A null distribution was performed to see the sampling distribution of the test statistic when the null hypothesis was true. THe graph shows the distribution of all mean differences on the scramble data so if there was no actual difference between the mean number of counts for smoking and drinking, this is the ditribution of mean differences we’d expect due to chance. What’s the porbabilty of getting a diff as big as 11 under random distribution under null is 1, can’t reject null.</p>
</div>
<div id="linear-regression" class="section level1">
<h1>3. Linear regression</h1>
<pre class="r"><code>lc$Result &lt;- as.character(lc$Result)
lc$Result[lc$Result == &quot;0&quot;] &lt;- &quot;No&quot;
lc$Result[lc$Result == &quot;1&quot;] &lt;- &quot;Yes&quot;
data.frame(Age_c=lc_new$Age-mean(lc_new$Age))</code></pre>
<pre><code>##          Age_c
## 1   -7.6271186
## 2  -15.6271186
## 3  -12.6271186
## 4  -14.6271186
## 5   25.3728814
## 6   -8.6271186
## 7   15.3728814
## 8  -20.6271186
## 9    2.3728814
## 10   9.3728814
## 11  -9.6271186
## 12 -24.6271186
## 13 -17.6271186
## 14 -14.6271186
## 15  -8.6271186
## 16  -3.6271186
## 17  -0.6271186
## 18 -23.6271186
## 19  19.3728814
## 20  30.3728814
## 21  12.3728814
## 22  -9.6271186
## 23 -20.6271186
## 24   1.3728814
## 25  34.3728814
## 26 -21.6271186
## 27  -5.6271186
## 28  -8.6271186
## 29  12.3728814
## 30  -2.6271186
## 31  -6.6271186
## 32  13.3728814
## 33   4.3728814
## 34  19.3728814
## 35 -16.6271186
## 36 -17.6271186
## 37  16.3728814
## 38  19.3728814
## 39  -9.6271186
## 40  -5.6271186
## 41   7.3728814
## 42   4.3728814
## 43  26.3728814
## 44  20.3728814
## 45  -3.6271186
## 46 -21.6271186
## 47 -11.6271186
## 48 -14.6271186
## 49  10.3728814
## 50  19.3728814
## 51  -0.6271186
## 52   1.3728814
## 53 -16.6271186
## 54  -7.6271186
## 55 -16.6271186
## 56  34.3728814
## 57  32.3728814
## 58   0.3728814
## 59   8.3728814</code></pre>
<pre class="r"><code>lc_new$Age_c &lt;- lc_new$Age - mean(lc_new$Age)
fit&lt;-lm(AreaQ ~ Loc*Age_c, data=lc_new)
summary(fit)</code></pre>
<pre><code>##
## Call:
## lm(formula = AreaQ ~ Loc * Age_c, data = lc_new)
##
## Residuals:
## Min 1Q Median 3Q Max
## -4.538 -1.619 -0.164 1.714 5.551
##
## Coefficients:
## Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 4.97253 0.42877 11.597 &lt;2e-16 ***
## LocRural 0.49066 0.63395 0.774 0.442
## Age_c -0.03404 0.02781 -1.224 0.226
## LocRural:Age_c -0.01133 0.03924 -0.289 0.774
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1
##
## Residual standard error: 2.42 on 55 degrees of freedom
## Multiple R-squared: 0.08375, Adjusted R-squared: 0.03377
## F-statistic: 1.676 on 3 and 55 DF, p-value: 0.1828</code></pre>
<pre class="r"><code>ggplot(lc_new, aes(x = Age_c, y =AreaQ , color = Loc)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = FALSE, fullrange = TRUE)</code></pre>
<p><img src="../project2_files/figure-html/unnamed-chunk-5-1.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>resids&lt;-fit$residuals; fitvals&lt;-fit$fitted.values
ggplot()+geom_point(aes(fitvals,resids))+geom_hline(yintercept=0, col=&quot;red&quot;)</code></pre>
<p><img src="../project2_files/figure-html/unnamed-chunk-5-2.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>bptest(fit)</code></pre>
<pre><code>## 
##  studentized Breusch-Pagan test
## 
## data:  fit
## BP = 0.47076, df = 3, p-value = 0.9253</code></pre>
<pre class="r"><code>ks.test(resids, &quot;pnorm&quot;, mean=0, sd(resids)) </code></pre>
<pre><code>## 
##  One-sample Kolmogorov-Smirnov test
## 
## data:  resids
## D = 0.050859, p-value = 0.998
## alternative hypothesis: two-sided</code></pre>
<pre class="r"><code>shapiro.test(resids)</code></pre>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  resids
## W = 0.98422, p-value = 0.6403</code></pre>
<pre class="r"><code>summary(fit)$coef[,1:2] #uncorrected SEs</code></pre>
<pre><code>##                   Estimate Std. Error
## (Intercept)     4.97252663 0.42876605
## LocRural        0.49065955 0.63395128
## Age_c          -0.03403514 0.02781267
## LocRural:Age_c -0.01132997 0.03924120</code></pre>
<pre class="r"><code>coeftest(fit, vcov = vcovHC(fit))[,1:2] #corrected SE</code></pre>
<pre><code>##                   Estimate Std. Error
## (Intercept)     4.97252663 0.45775623
## LocRural        0.49065955 0.64729531
## Age_c          -0.03403514 0.02473327
## LocRural:Age_c -0.01132997 0.03549120</code></pre>
<pre class="r"><code>(sum((lc_new$AreaQ-mean(lc_new$AreaQ))^2)-sum(fit$residuals^2))/sum((lc_new$AreaQ-mean(lc_new$AreaQ))^2)</code></pre>
<pre><code>## [1] 0.08374764</code></pre>
<p>The results showed that location and age did not have a significant effect on the area of the disease. 0.49066 showed that when age was zero, people in rural location has an area that was 0.49066 higher than people in the city area. -0.03404 showed when location was 0, older individuals were more likely to get cancer. -0.01133 was the difference in slope of people between people who live in the rural or city area for the effect of aging on getting cancer so we cannot reject the null hypothesis that smoking has an effect on both the groups of aging and getting cancer.</p>
<p>One assumption for linear regression is linearity which was met since there were no curves on the scatterplot. The test for normality was done both by the Kolmogorov-Smirnov test and the Shapiro-Wilk normality test which had a null hypothesis that claimed that the true distribution is normal. Since both had p values that were greater than 0.05, we fail to reject the null hypothesis and therefore the the distribution is normal and the assumption of normality is met. Another assumption of Homoscedasticity, a bp test was perfomened that tested the null hypothesis that the data was homoscedasticity and has equal variance and since the p value was not less than 0.05 and was in fact 0.9253, the data failed to reject the null hypothesis and upheld homoscedasticity.</p>
<p>The original regression results showed the standard errors under the assumption that the null hypothesis was true. When the regression results were recomputed with robust standard errors via coeftest(…,vcov=vcovHC(…)), to acquire corrected standard errors robust to violations of homoskedasticity. The new results of the robust SE remained approximately the same.</p>
<p>The proportion of the variation in the outcome that was explained by your the model was 0.08374764.</p>
</div>
<div id="regression-model-with-bootstrapping-se" class="section level1">
<h1>4. Regression Model with Bootstrapping SE</h1>
<pre class="r"><code>coeftest(fit)[,1:2]## Normal-theory SEs</code></pre>
<pre><code>##                   Estimate Std. Error
## (Intercept)     4.97252663 0.42876605
## LocRural        0.49065955 0.63395128
## Age_c          -0.03403514 0.02781267
## LocRural:Age_c -0.01132997 0.03924120</code></pre>
<pre class="r"><code>coeftest(fit, vcov=vcovHC(fit))[,1:2] ## Robust SEs</code></pre>
<pre><code>##                   Estimate Std. Error
## (Intercept)     4.97252663 0.45775623
## LocRural        0.49065955 0.64729531
## Age_c          -0.03403514 0.02473327
## LocRural:Age_c -0.01132997 0.03549120</code></pre>
<pre class="r"><code>boot_dat&lt;- sample_frac(lc_new, replace=T)
samp_distn&lt;-replicate(5000, {
boot_dat &lt;- sample_frac(lc_new, replace=T) #bootstrap your data
fit &lt;- lm(AreaQ ~ Loc*Age_c, data=boot_dat) #fit model
coef(fit) #save coefs
})
samp_distn %&gt;% t %&gt;% as.data.frame %&gt;% summarize_all(sd)</code></pre>
<pre><code>##   (Intercept)  LocRural     Age_c LocRural:Age_c
## 1   0.4518377 0.6364776 0.0251151     0.03527756</code></pre>
<pre class="r"><code>samp_distn %&gt;% t %&gt;% as.data.frame %&gt;% gather %&gt;% group_by(key) %&gt;% summarize(lower=quantile(value,.025), upper=quantile(value,.975))</code></pre>
<pre><code>## # A tibble: 4 x 3
##   key              lower  upper
##   &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;
## 1 (Intercept)     4.09   5.83  
## 2 Age_c          -0.0813 0.0191
## 3 LocRural       -0.729  1.75  
## 4 LocRural:Age_c -0.0852 0.0552</code></pre>
<pre class="r"><code>fit&lt;-lm(AreaQ ~ Loc*Age_c, data=lc_new)
resids&lt;-fit$residuals
fitted&lt;-fit$fitted.values
resid_resamp&lt;-replicate(5000,{
new_resids&lt;-sample(resids,replace=TRUE)
newdat&lt;-lc_new
newdat$new_y&lt;-fitted+new_resids
fit&lt;-lm(new_y ~ Loc*Age_c, data = newdat)
coef(fit)
})
resid_resamp%&gt;%t%&gt;%as.data.frame%&gt;%summarize_all(sd)</code></pre>
<pre><code>##   (Intercept)  LocRural      Age_c LocRural:Age_c
## 1   0.4104408 0.6145152 0.02678997     0.03817309</code></pre>
<pre class="r"><code>resid_resamp%&gt;%t%&gt;%as.data.frame%&gt;%gather%&gt;%group_by(key)%&gt;% summarize(lower=quantile(value,.025), upper=quantile(value,.975))</code></pre>
<pre><code>## # A tibble: 4 x 3
##   key              lower  upper
##   &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;
## 1 (Intercept)     4.15   5.78  
## 2 Age_c          -0.0864 0.0170
## 3 LocRural       -0.700  1.74  
## 4 LocRural:Age_c -0.0853 0.0656</code></pre>
<p>The bootstrapping showed that we’re 95% sure that the true intercept was between 4.08495305 and 5.82291651 while the estimate was 0.4391239. The same theory can be applied to thinking that the true location of being rural was between -0.079558916 and 1.71249059 (it was estimated to be 0.6243009), the true slope for age was between -0.07952698 and 0.01673242 (it was estimated to be 0.02464783), and the true interaction estimation between location and age was between -0.07915423 and 0.05369964 (it was estimate to be 0.03428608). The same theory applied when bootstrapping was done on the residuals. These values approximately matched the SE from the original and robust results.</p>
</div>
<div id="logistic-regression" class="section level1">
<h1>5. Logistic regression</h1>
<pre class="r"><code>lc_again&lt;-lc%&gt;%mutate(y=ifelse(Result==&quot;Yes&quot;,1,0))
lc_again$Result &lt;- as.numeric(lc_again$Result)
fit_log &lt;- glm(y~Alkhol+Smokes, data = lc_again, family = binomial(link = logit))
coeftest(fit_log)</code></pre>
<pre><code>##
## z test of coefficients:
##
## Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept) -9.039099 3.180996 -2.8416 0.004489 **
## Alkhol 3.022887 1.047269 2.8864 0.003896 **
## Smokes -0.027128 0.104011 -0.2608 0.794233
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1</code></pre>
<pre class="r"><code>exp(coef(fit_log))</code></pre>
<pre><code>##  (Intercept)       Alkhol       Smokes 
## 1.186777e-04 2.055054e+01 9.732367e-01</code></pre>
<pre class="r"><code>logistic&lt;-function(x){exp(x)/(1+exp(x))}
probs&lt;-predict(fit_log,type=&quot;response&quot;) #get predicted probs from the model
table(predict=as.numeric(probs&gt;.5),truth=lc_again$y)%&gt;%addmargins</code></pre>
<pre><code>##        truth
## predict  0  1 Sum
##     0   30  4  34
##     1    1 24  25
##     Sum 31 28  59</code></pre>
<pre class="r"><code>odds &lt;- function(p) p/(1 - p)
p &lt;- seq(0, 1, by = 0.1)
cbind(p, odds = odds(p)) %&gt;% round(4)</code></pre>
<pre><code>##         p   odds
##  [1,] 0.0 0.0000
##  [2,] 0.1 0.1111
##  [3,] 0.2 0.2500
##  [4,] 0.3 0.4286
##  [5,] 0.4 0.6667
##  [6,] 0.5 1.0000
##  [7,] 0.6 1.5000
##  [8,] 0.7 2.3333
##  [9,] 0.8 4.0000
## [10,] 0.9 9.0000
## [11,] 1.0    Inf</code></pre>
<pre class="r"><code>logit &lt;- function(p) log(odds(p))
lc_again$logit &lt;- predict(fit_log)
lc_again$y &lt;- factor(lc_again$y, levels = c(1, 0))
ggplot(lc_again, aes(logit, fill = y)) + geom_density(alpha = 0.4) + geom_vline(xintercept = 0, lty = 2)</code></pre>
<p><img src="../project2_files/figure-html/unnamed-chunk-7-1.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>lc_again$prob &lt;- predict(fit_log, type = &quot;response&quot;)
lc_again$y &lt;- as.factor(lc_again$y)
sens&lt;-function(p,data=lc_again, y=y) mean(lc_again[lc_again$y==1,]$prob&gt;p)
spec&lt;-function(p,data=lc_again, y=y) mean(lc_again[lc_again$y==0,]$prob&lt;p)
sensitivity&lt;-sapply(seq(0,1,.01),sens,lc_again)
specificity&lt;-sapply(seq(0,1,.01),spec,lc_again)
ROC1&lt;-data.frame(sensitivity,specificity,cutoff=seq(0,1,.01))
ROC1%&gt;%gather(key,rate,-cutoff)%&gt;%ggplot(aes(cutoff,rate,color=key))+geom_path()+ geom_vline(xintercept=c(.1,.5,.9),lty=2,color=&quot;gray50&quot;)</code></pre>
<p><img src="../project2_files/figure-html/unnamed-chunk-7-2.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>ROC1$TPR&lt;-sensitivity
ROC1$FPR&lt;-1-specificity
ROC1%&gt;%ggplot(aes(FPR,TPR))+geom_path(size=1.5)+geom_segment(aes(x=0,y=0,xend=1,yend=1), lty=2)+scale_x_continuous(limits = c(0,1))</code></pre>
<p><img src="../project2_files/figure-html/unnamed-chunk-7-3.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>widths &lt;- diff(ROC1$FPR)
heights &lt;- vector()
for (i in 1:100) heights[i] &lt;- ROC1$TPR[i] + ROC1$TPR[i + 1]
AUC &lt;- sum(heights * widths/2)
AUC %&gt;% round(4)</code></pre>
<pre><code>## [1] -0.9821</code></pre>
<pre class="r"><code>set.seed(1234)
k=10


lc_again$Result&lt;-NULL #remove this variable for now
fit &lt;- glm(y~Alkhol+Smokes,data=lc_again,family=&quot;binomial&quot;) #fit model
prob &lt;- predict(fit,type=&quot;response&quot;) #get predicted probabilities

class_diag &lt;- function(probs,truth){
#CONFUSION MATRIX: CALCULATE ACCURACY, TPR, TNR, PPV
tab&lt;-table(factor(probs&gt;.5,levels=c(&quot;FALSE&quot;,&quot;TRUE&quot;)),truth)
acc=sum(diag(tab))/sum(tab)
sens=tab[2,2]/colSums(tab)[2]
spec=tab[1,1]/colSums(tab)[1]
ppv=tab[2,2]/rowSums(tab)[2]
if(is.numeric(truth)==FALSE &amp; is.logical(truth)==FALSE) truth&lt;-as.numeric(truth)-1
#CALCULATE EXACT AUC
ord&lt;-order(probs, decreasing=TRUE)
probs &lt;- probs[ord]; truth &lt;- truth[ord]
TPR=cumsum(truth)/max(1,sum(truth))
FPR=cumsum(!truth)/max(1,sum(!truth))
dup&lt;-c(probs[-1]&gt;=probs[-length(probs)], FALSE)
TPR&lt;-c(0,TPR[!dup],1); FPR&lt;-c(0,FPR[!dup],1)
n &lt;- length(TPR)
auc&lt;- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
data.frame(acc,sens,spec,ppv,auc)
} 
class_diag(prob, lc_again$y)</code></pre>
<pre><code>##         acc      sens      spec       ppv       auc
## 0 0.9152542 0.9677419 0.8571429 0.8823529 0.9821429</code></pre>
<pre class="r"><code>library(pROC)
auc(lc_again$y,prob)</code></pre>
<pre><code>## Area under the curve: 0.9821</code></pre>
<pre class="r"><code>set.seed(1234)
k=10 #choose number of folds
data&lt;-lc_again[sample(nrow(lc_again)),] #randomly order rows
folds&lt;-cut(seq(1:nrow(lc_again)),breaks=k,labels=F) #create folds
diags&lt;-NULL
for(i in 1:k){
## Create training and test sets
train&lt;-data[folds!=i,]
test&lt;-data[folds==i,]
truth&lt;-test$y ## Truth labels for fold i
## Train model on training set (all but fold i)
fit&lt;-glm(y~Alkhol+Smokes,data=train,family=&quot;binomial&quot;)
## Test model on test set (fold i)
probs&lt;-predict(fit,newdata = test,type=&quot;response&quot;)
## Get diagnostics for fold i
diags&lt;-rbind(diags,class_diag(probs,truth))
}
summarize_all(diags,mean)</code></pre>
<pre><code>##         acc  sens spec ppv     auc
## 1 0.8833333 0.875 0.86 NaN 0.98125</code></pre>
<pre class="r"><code>apply(diags,2,mean)</code></pre>
<pre><code>##       acc      sens      spec       ppv       auc 
## 0.8833333 0.8750000 0.8600000       NaN 0.9812500</code></pre>
<p>Drinking alcohol have a significant effect on the probablitly that someone would develope cancer will smoking did not since the p value of smoking is not less than 0.05. We expect the log-odds of developing cancer to go up by 3.022887 for every one unit increase in drinking. The probability that someone had cancer when they had a smoke or drink count to the power of 0 would be -9.039099.</p>
<p>Sensitivity was the true positive rate (TPR) which in this case is the probability of detecting how many people really had lung cancer which was 24/28 = 0.8571. Specificity was the true negative rate (TNR) which is the probability of a negative test for healthy people which was 30/31 = 0.9677. Precision (PPV) was the proportion classified malignant who actually were which was 24/25 = 0.96.</p>
<p>Given the AUC of -0.9821, so probabilty someone with cancer has a -0.9821 chance of having a higher probabilty than someone without cancer. Overall the score was quite low and the AUC would be considered bad.</p>
<p>In the denist yplot, logit &gt; 0 meant we predict malignant red to the right: TP; blue to the right, FP. If logit &lt; 0, it meant we predict benign blue to the right: TN; red to the right, FN.</p>
<pre class="r"><code>lc$Result[lc$Result == &quot;No&quot;] &lt;- &quot;0&quot;
lc$Result[lc$Result == &quot;Yes&quot;] &lt;- &quot;1&quot;
lc$Result &lt;- as.numeric(lc$Result)</code></pre>
</div>
<div id="lasso" class="section level1">
<h1>6.Lasso</h1>
<pre class="r"><code>y&lt;-as.matrix(lc$Result) #grab response
x&lt;-model.matrix(Result~., data=lc)[,-1] #grab predictors
head(x)</code></pre>
<pre><code>## NameAlex NameAnna NameBarbra NameBarbra NameCamela
NameCharlize NameCharlton NameCristiano
## 1 0 0 0 0 0 0 0 0
## 2 0 0 0 0 0 0 0 0
## 3 0 0 0 0 1 0 0 0
## 4 1 0 0 0 0 0 0 0
## 5 0 0 0 0 0 0 0 0
## 6 0 0 0 0 0 0 0 1
## NameDiane NameDiego NameDustin NameEllen NameErnest
NameFaye NameFredric NameGene
## 1 0 0 0 0 0 0 0 0
## 2 0 0 0 0 0 0 0 0
## 3 0 0 0 0 0 0 0 0
## 4 0 0 0 0 0 0 0 0
## 5 0 1 0 0 0 0 0 0
## 6 0 0 0 0 0 0 0 0
## NameGlenda NameGregory NameGwyneth NameHalle NameHenry
NameJack NameJane NameJane
## 1 0 0 0 0 0 0 0 0
## 2 0 0 0 0 0 0 0 0
## 3 0 0 0 0 0 0 0 0
## 4 0 0 0 0 0 0 0 0
## 5 0 0 0 0 0 0 0 0
## 6 0 0 0 0 0 0 0 0
## NameJessica NameJoan NameJohn NameJohn NameKatharine
NameKathy NameLee NameMaggie
## 1 0 0 1 0 0 0 0 0
## 2 0 0 1 0 0 0 0 0
## 3 0 0 0 0 0 0 0 0
## 4 0 0 0 0 0 0 0 0
## 5 0 0 0 0 0 0 0 0
## 6 0 0 0 0 0 0 0 0
## NameMarlon NameMaximilian NameMihail NameNicole
NameNicole NamePaul NamePeter NameRay
## 1 0 0 0 0 0 0 0 0
## 2 0 0 0 0 0 0 0 0
## 3 0 0 0 0 0 0 0 0
## 4 0 0 0 0 0 0 0 0
## 5 0 0 0 0 0 0 0 0
## 6 0 0 0 0 0 0 0 0
## NameRex NameRichard NameRobert NameRod NameSally
NameSidney NameSissy NameYul SurnameBates
## 1 0 0 0 0 0 0 0 0 0
## 2 0 0 0 0 0 0 0 0 0
## 3 0 0 0 0 0 0 0 0 0
## 4 0 0 0 0 0 0 0 0 0
## 5 0 0 0 0 0 0 0 0 0
## 6 0 0 0 0 0 0 0 0 0
## SurnameBerry SurnameBorgnine SurnameBrando
SurnameBrynner SurnameBurstyn SurnameConstantine
## 1 0 0 0 0 0 0
## 2 0 0 0 0 0 1
## 3 0 0 0 0 0 0
## 4 0 0 0 0 0 0
## 5 0 0 0 0 0 0
## 6 0 0 0 0 0 0
## SurnameCrawford SurnameDreyfuss SurnameDunaway
SurnameDuvall SurnameField SurnameFinch
## 1 0 0 0 0 0 0
## 2 0 0 0 0 0 0
## 3 0 0 0 0 0 0
## 4 0 0 0 0 0 0
## 5 0 0 0 0 0 0
## 6 0 0 0 0 0 0
## SurnameFonda SurnameGuinness SurnameHackman
SurnameHarrison SurnameHenry SurnameHepburn
## 1 0 0 0 0 0 0
## 2 0 0 0 0 0 0
## 3 0 0 0 0 0 0
## 4 0 0 0 0 0 0
## 5 0 0 0 0 0 0
## 6 0 0 0 0 0 0
## SurnameHeston SurnameHoffman SurnameJackson
SurnameKeaton SurnameKidman SurnameLange
## 1 0 0 0 0 0 0
## 2 0 0 0 0 0 0
## 3 0 0 0 0 0 0
## 4 0 0 0 0 0 0
## 5 0 0 0 0 0 0
## 6 0 0 0 0 0 0
## SurnameLemmon SurnameMagnani SurnameMaradona
SurnameMarch SurnameMarvin SurnameMilland
## 1 0 0 0 0 0 0
## 2 0 0 0 0 0 0
## 3 0 0 0 0 0 0
## 4 0 0 0 0 0 0
## 5 0 0 1 0 0 0
## 6 0 0 0 0 0 0
## SurnameNicholson SurnamePaltrow SurnamePeck
SurnamePoitier SurnameRonaldo SurnameSchell
## 1 0 0 0 0 0 0
## 2 0 0 0 0 0 0
## 3 0 0 0 0 0 0
## 4 0 0 0 0 0 0
## 5 0 0 0 0 0 0
## 6 0 0 0 0 1 0
## SurnameScofield SurnameSmith SurnameSpacek
SurnameSteiger SurnameStreisand SurnameTal
## 1 0 0 0 0 0 0
## 2 0 0 0 0 0 0
## 3 0 0 0 0 0 0
## 4 0 0 0 0 0 0
## 5 0 0 0 0 0 0
## 6 0 0 0 0 0 0
## SurnameTelles SurnameTheron SurnameWayne SurnameWick
SurnameWyman Age Smokes AreaQ Alkhol
## 1 0 0 0 1 0 35 3 5 4
## 2 0 0 0 0 0 27 20 2 5
## 3 0 0 0 0 0 30 0 5 2
## 4 1 0 0 0 0 28 0 8 1
## 5 0 0 0 0 0 68 4 5 6
## 6 0 0 0 0 0 34 0 10 0</code></pre>
<pre class="r"><code>cv&lt;-cv.glmnet(x,y,family=&quot;binomial&quot;)
lasso&lt;-glmnet(x,y,family=&quot;binomial&quot;,lambda=cv$lambda.1se)
coef(lasso)</code></pre>
<pre><code>## 101 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                              s0
## (Intercept)        -5.994242592
## NameAlex            .          
## NameAnna            .          
## NameBarbra          .          
## NameBarbra          .          
## NameCamela          .          
## NameCharlize        2.043708970
## NameCharlton        .          
## NameCristiano       .          
## NameDiane           .          
## NameDiego           .          
## NameDustin          .          
## NameEllen           .          
## NameErnest          .          
## NameFaye           -1.088280021
## NameFredric         .          
## NameGene            .          
## NameGlenda          .          
## NameGregory         .          
## NameGwyneth         .          
## NameHalle           .          
## NameHenry           .          
## NameJack            .          
## NameJane            .          
## NameJane            .          
## NameJessica         .          
## NameJoan            .          
## NameJohn            2.100887223
## NameJohn            .          
## NameKatharine       .          
## NameKathy           .          
## NameLee             .          
## NameMaggie          .          
## NameMarlon         -0.116354783
## NameMaximilian      .          
## NameMihail          .          
## NameNicole          .          
## NameNicole          .          
## NamePaul            .          
## NamePeter           .          
## NameRay             .          
## NameRex             .          
## NameRichard         .          
## NameRobert          .          
## NameRod             .          
## NameSally           .          
## NameSidney          .          
## NameSissy           .          
## NameYul             .          
## SurnameBates        .          
## SurnameBerry        .          
## SurnameBorgnine     .          
## SurnameBrando      -0.001456986
## SurnameBrynner      .          
## SurnameBurstyn      .          
## SurnameConstantine  .          
## SurnameCrawford     .          
## SurnameDreyfuss     .          
## SurnameDunaway     -0.180377411
## SurnameDuvall       .          
## SurnameField        .          
## SurnameFinch        .          
## SurnameFonda        .          
## SurnameGuinness     .          
## SurnameHackman      .          
## SurnameHarrison     .          
## SurnameHenry        .          
## SurnameHepburn      .          
## SurnameHeston       .          
## SurnameHoffman      .          
## SurnameJackson      .          
## SurnameKeaton       .          
## SurnameKidman       .          
## SurnameLange        .          
## SurnameLemmon       0.005708238
## SurnameMagnani      .          
## SurnameMaradona     .          
## SurnameMarch        .          
## SurnameMarvin       .          
## SurnameMilland      .          
## SurnameNicholson    .          
## SurnamePaltrow      .          
## SurnamePeck         .          
## SurnamePoitier      .          
## SurnameRonaldo      .          
## SurnameSchell       .          
## SurnameScofield     .          
## SurnameSmith        .          
## SurnameSpacek       .          
## SurnameSteiger      .          
## SurnameStreisand    .          
## SurnameTal          .          
## SurnameTelles       .          
## SurnameTheron       0.258223396
## SurnameWayne        .          
## SurnameWick         0.909869647
## SurnameWyman        .          
## Age                 0.131262511
## Smokes              .          
## AreaQ              -0.634313144
## Alkhol              1.019805282</code></pre>
<pre class="r"><code>set.seed(1234)
k=10
data &lt;- lc %&gt;% sample_frac #put rows of dataset in random order
folds &lt;- ntile(1:nrow(data),n=10) #create fold labels
diags&lt;-NULL
for(i in 1:k){
train &lt;- data[folds!=i,] #create training set (all but fold i)
test &lt;- data[folds==i,] #create test set (just fold i)
truth &lt;- test$Result #save truth labels from fold i
fit &lt;- glm(Result~Age+Smokes+Alkhol+AreaQ,data=train, family=&quot;binomial&quot;)
probs &lt;- predict(fit, newdata=test, type=&quot;response&quot;)
diags&lt;-rbind(diags,class_diag(probs,truth))
}
diags%&gt;%summarize_all(mean)</code></pre>
<pre><code>##    acc sens spec  ppv    auc
## 1 0.95  0.9 0.95 0.98 0.9375</code></pre>
<p>This model’s out-of-sample accuracy of 0.9375 was close to rthe auc of logistic regression in part 5 which was 0.9812500. Given these results, the 0.98 was more accurate. Ignoring the names which had no real life effect on cancer, alkhol appeared to be an important predictor of cancer. Age and AreaQ were retained as well.</p>
<p>…</p>
</div>

              <hr>
              <div class="related-posts">
                <h5>Related Posts</h5>
                
              </div>
            </div>
          </div>
          <hr>
        <div class="disqus">
  <div id="disqus_thread"></div>
  <script type="text/javascript">

    (function() {
      
      
      if (window.location.hostname == "localhost")
        return;

      var disqus_shortname = '';
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>
        </div>
      </div>
      
    </div>

    
    <footer>
  <div id="footer">
    <div class="container">
      <p class="text-muted">&copy; All rights reserved. Powered by <a href="https://gohugo.io/">Hugo</a> and
      <a href="http://www.github.com/nurlansu/hugo-sustain/">sustain</a> with ♥</p>
    </div>
  </div>
</footer>
<div class="footer"></div>


<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>

<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
<script src="../js/docs.min.js"></script>
<script src="../js/main.js"></script>

<script src="../js/ie10-viewport-bug-workaround.js"></script>


    
  </body>
</html>
